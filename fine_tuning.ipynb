{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - process conversation data and extract prompts & responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_prompt_response_pairs(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        conversations = json.load(infile)\n",
    "        for conversation in conversations:\n",
    "            for turn in conversation['turns']:\n",
    "                if turn['speaker'] == 'user':\n",
    "                    prompt = turn['text']\n",
    "                elif turn['speaker'] == 'assistant':\n",
    "                    response = turn['text']\n",
    "                    pair = {'prompt': prompt, 'response': response}\n",
    "                    json.dump(pair, outfile)\n",
    "                    outfile.write('\\n')\n",
    "\n",
    "extract_prompt_response_pairs('conversations.json', 'prompt_response_pairs.jsonl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step was to obtain the original Cohere Aya-8B model and load it using 8-bit precision with the help of the bits and bytes library. Using the model's original precision would have led to out-of-memory errors, so applied quantization to reduce the precision to 8-bit. After loading the model with reduced precision, I ensured that it was set to training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets peft huggingface_hub\n",
    "pip install accelerate bitsandbytes\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import get_peft_model, LoraConfig, PeftType\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/aya-23-8B\")\n",
    "\n",
    "# No need to add special tokens since they are already in the vocabulary\n",
    "\n",
    "# Load the model with 8-bit precision using bitsandbytes\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"CohereForAI/aya-23-8B\",\n",
    "    device_map=\"auto\",  # Automatically handle device mapping\n",
    "    load_in_8bit=True,  # Load the model in 8-bit precision to save memory\n",
    "    offload_folder=\"./offload\",  # Folder to store offloaded model parts\n",
    "    torch_dtype=\"float16\"  # Use 16-bit precision for floating-point operations\n",
    ")\n",
    "\n",
    "# Resize token embeddings to account for added special tokens\n",
    "# If you are sure no special tokens need to be added, you might not need to resize\n",
    "# model.resize_token_embeddings(len(tokenizer))  # Not needed if no new tokens are added\n",
    "\n",
    "# Ensure the model is in training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I loaded my dataset from the finetuning_data.jsonl file, which includes the prompts and responses. Since we are fine-tuning a chat model, it's crucial to use specific tokens in the vocabulary to structure the chat conversation for the final output. These tokens include the BOS token, start-of-turn token, user token, chatbot token, and end-of-turn token. During tokenization, I place the prompt inside the user token and the response inside the chatbot token. This approach helps the model learn to differentiate between the prompt and the corresponding response. To achieve this, I use a dedicated tokenize function. Additionally, I set the tokenizer's maximum length to 512 tokens to prevent memory issues, especially since we're using a single GPU with 40 GB of GPU RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a jsonl file\n",
    "dataset = load_dataset(\"json\", data_files=\"finetuning_data.jsonl\")\n",
    "\n",
    "# Tokenize function using the existing special tokens in the vocabulary\n",
    "def tokenize_function(examples):\n",
    "    input_texts = [\n",
    "        f\"<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>{prompt}<|END_OF_TURN_TOKEN|>\"\n",
    "        f\"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{response}<|END_OF_TURN_TOKEN|>\"\n",
    "        for prompt, response in zip(examples['prompt'], examples['response'])\n",
    "    ]\n",
    "    encoding = tokenizer(input_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"].copy()\n",
    "    return encoding\n",
    "\n",
    "# Tokenize the entire dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"prompt\", \"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I used LoRa (Low-Rank Adaptation) for fine-tuning. LoRa enables us to modify only a subset of the model's weights, which helps us achieve our fine-tuning goals without the need to retrain the entire model, making the process less computationally intensive. I configured LoRa by setting parameters like R, LoRa alpha, and LoRa dropout. To prevent memory issues, I reduced LoRa alpha from 32 to 16 and set LoRa dropout to 0.1. Once the LoRa configuration was defined, I integrated it with the model using parameter-efficient fine-tuning, accomplished with the get_peft_model function, which takes both the model and LoRa configurations as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set up LoRA configuration for PEFT\n",
    "lora_config = LoraConfig(\n",
    "    peft_type=PeftType.LORA,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "# Step 8: Integrate the model with LoRA using PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Optionally disable gradient checkpointing to resolve conflicts\n",
    "# model.gradient_checkpointing_enable()  # Disable this if it causes conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to define the training arguments. I set the maximum number of training steps to 100, as I noticed performance degradation beyond this point in earlier experiments. To minimize the risk of out-of-memory errors, I used a batch size of 1 and implemented gradient accumulation after every 16 steps. While there are many other parameters available for tuning, these were the key ones for this particular setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Define training arguments with max_steps set to 100\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=100,  # Limit training to 100 steps\n",
    "    per_device_train_batch_size=1,  # Set the batch size\n",
    "    gradient_accumulation_steps=16,  # To handle memory issues\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=50,  # Save checkpoint every 50 steps\n",
    "    save_total_limit=2,  # Keep only the latest checkpoints\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,  # Evaluate every 50 steps\n",
    "    remove_unused_columns=False,\n",
    "    fp16=False,  # Disable mixed precision to avoid conflicts\n",
    ")\n",
    "\n",
    "# Step 10: Initialize the Trainer with PEFT\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "\n",
    "# Step 11: Fine-tune the model using PEFT and LoRA\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
